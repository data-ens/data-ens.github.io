- speaker: "Lénaïc Chizat"
  date: 11 January 2024
  time: 12h30-13h30 (Paris time)
  room: "Amphi Jaures (29 Rue d'Ulm)"
  affiliation: "EPFL"
  url: "https://lchizat.github.io/"
  title: "Steering Deep Feature Learning with Backward Aligned Feature Updates"
  abstract: "Deep learning succeeds by doing hierarchical feature learning... but tuning Hyper-Parameters (HP) such as initialization scales, learning rates, etc., only give indirect control over this behavior. In the first part of the talk, I will review some theoretical approaches — often based on complicated infinite width dynamics — that have been proposed to predict feature learning and its relation to HP scalings. I will then propose a simpler and more flexible approach: it uses the alignment between the feature updates and the backward pass as a pivotal notion to predict, measure and control feature learning. Our key finding is that, when alignment holds, the magnitude of feature updates after one SGD step is related to the magnitude of the forward and backward passes by a simple and general formula. As illustration of these ideas, I will discuss the feature learning behavior of ReLU MLPs and ResNets in the infinite width and depth limit.

  Talk based on: https://arxiv.org/abs/2311.18718 (jt work with Praneeth Netrapalli)"
  slides: nothing
  video: nothing
