
- speaker: "Mathieu Blondel"
  date: June 8th, 2018
  time: 10h-11h
  room: "W, DMA, ENS (45 rue d'Ulm, 4th floor)"
  affiliation: "NTT (Kyoto)"
  title: "Smoothing / Regularization Techniques for Probabilistic and Structured Classification"
  abstract: "In this talk, I will present how smoothing / regularization techniques can be used to turn a prediction function that outputs a hard decision (e.g top-scoring class or top-scoring sequence) into a differentiable \"soft\" decision with a probabilistic perspective. I will present a novel principled loss family to learn such \"soft\" prediction functions from training data. In addition, I will show how to use them as a layer in a neural network trained end-to-end by backpropagation. Finally, I will present  experimental results on natural language processing applications such as named entity recognition, dependency parsing and machine translation. <br> Joint work with Vlad Niculae, Andr√© Martins, Claire Cardie, Arthur Mensch <br> The talk will be a condensed overview of the following papers: <br> [1] https://arxiv.org/abs/1802.04223 (ICML 2018) <br> [2] https://arxiv.org/abs/1802.03676 (ICML 2018) <br> [3] https://arxiv.org/abs/1805.09717 (arXiv preprint)"
  url: "http://mblondel.org/"
  
- speaker: "Alberto Bietti"
  date: June 8th, 2018
  time: 11h-12h
  room: "W, DMA, ENS (45 rue d'Ulm, 4th floor)"
  affiliation: "Inria"
  title: "Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations"
  abstract: "The success of deep convolutional architectures is often attributed in part to their ability to learn multiscale and invariant representations of natural signals. However, a precise study of these properties and how they affect learning guarantees is still missing. In this work, we consider deep convolutional representations of signals; we study their invariance to translations and to more general groups of transformations, their stability to the action of diffeomorphisms, and their ability to preserve signal information. This analysis is carried by introducing a multilayer kernel based on convolutional kernel networks and by studying the geometry induced by the kernel mapping. We then characterize the corresponding reproducing kernel Hilbert space (RKHS), showing that it contains a large class of convolutional neural networks with homogeneous activation functions. This analysis allows us to separate data representation from learning, and to provide a canonical measure of model complexity, the RKHS norm, which controls both stability and generalization of any learned model. In addition to models in the constructed RKHS, our stability analysis also applies to convolutional networks with generic activations such as rectified linear units, and we discuss its relationship with recent generalization bounds based on spectral norms. <br> https://arxiv.org/abs/1706.03078"
  url: "http://alberto.bietti.me/"