
- speaker: "Matthew Colbrook"
  date: May 10th, 2019
  time: 11h00-12h00
  room: "W, DMA, ENS (45 rue d'Ulm, 4th floor)"
  affiliation: "Cambridge University"
  title: "Can neural networks always be trained? On the boundaries of deep learning "
  abstract: "Deep learning has emerged as a competitive new tool in image reconstruction. However, recent results demonstrate such methods are typically highly unstable â€“ tiny, almost undetectable perturbations cause severe artefacts in the reconstruction, a major concern in practice. This is paradoxical given the existence of stable state-of-the-art methods for these problems. Thus, approximation theoretical results non-constructively imply the existence of stable and accurate neural networks. Hence the fundamental question: Can we explicitly construct/train stable and accurate neural networks for image reconstruction? I will discuss two results in this direction. The first is a negative result, saying such constructions are in general impossible, even given access to the solutions of common optimisation algorithms such as basis pursuit. The second is a positive result, saying that under sparsity assumptions, such neural networks can be constructed. These neural networks are stable and theoretically competitive with state-of-the-art results from other methods. Numerical examples of competitive performance are also provided. "
  url: "https://www.maths.cam.ac.uk/postgrad/cca/people/mjc249.html"
  