- speaker: "Juan Peypouquet"
  date: July 11th, 2018
  time: 11h-12h
  room: "R3, DMA, ENS (45 rue d'Ulm, basement)"
  affiliation: "Universidad Técnica Federico Santa María (Chile)"
  title: "Inertial proximal algorithms for maximally monotone operators"
  abstract: "We present a Regularized Inertial Proximal Algorithm to solve convex optimization problems and variational inequalities. It is obtained by means of a convenient finite-difference discretization of a second-order differential equation with vanishing damping, governed by the Yosida regularization of a maximally monotone operator with time-varying index. These systems are the counterpart to accelerated forward–backward algorithms in the context of maximally monotone operators. A simple example illustrates the behavior of these systems compared with some of their close relatives."
  url: "http://jpeypou.mat.utfsm.cl/"

- speaker: "Mathieu Blondel"
  date: June 8th, 2018
  time: 10h-11h
  room: "W, DMA, ENS (45 rue d'Ulm, 4th floor)"
  affiliation: "NTT (Kyoto)"
  title: "Smoothing / Regularization Techniques for Probabilistic and Structured Classification"
  abstract: "In this talk, I will present how smoothing / regularization techniques can be used to turn a prediction function that outputs a hard decision (e.g top-scoring class or top-scoring sequence) into a differentiable \"soft\" decision with a probabilistic perspective. I will present a novel principled loss family to learn such \"soft\" prediction functions from training data. In addition, I will show how to use them as a layer in a neural network trained end-to-end by backpropagation. Finally, I will present  experimental results on natural language processing applications such as named entity recognition, dependency parsing and machine translation. <br> Joint work with Vlad Niculae, André Martins, Claire Cardie, Arthur Mensch <br> The talk will be a condensed overview of the following papers: <br> [1] https://arxiv.org/abs/1802.04223 (ICML 2018) <br> [2] https://arxiv.org/abs/1802.03676 (ICML 2018) <br> [3] https://arxiv.org/abs/1805.09717 (arXiv preprint)"
  url: "http://mblondel.org/"
  
- speaker: "Alberto Bietti"
  date: June 8th, 2018
  time: 11h-12h
  room: "W, DMA, ENS (45 rue d'Ulm, 4th floor)"
  affiliation: "Inria"
  title: "Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations"
  abstract: "The success of deep convolutional architectures is often attributed in part to their ability to learn multiscale and invariant representations of natural signals. However, a precise study of these properties and how they affect learning guarantees is still missing. In this work, we consider deep convolutional representations of signals; we study their invariance to translations and to more general groups of transformations, their stability to the action of diffeomorphisms, and their ability to preserve signal information. This analysis is carried by introducing a multilayer kernel based on convolutional kernel networks and by studying the geometry induced by the kernel mapping. We then characterize the corresponding reproducing kernel Hilbert space (RKHS), showing that it contains a large class of convolutional neural networks with homogeneous activation functions. This analysis allows us to separate data representation from learning, and to provide a canonical measure of model complexity, the RKHS norm, which controls both stability and generalization of any learned model. In addition to models in the constructed RKHS, our stability analysis also applies to convolutional networks with generic activations such as rectified linear units, and we discuss its relationship with recent generalization bounds based on spectral norms. <br> https://arxiv.org/abs/1706.03078"
  url: "http://alberto.bietti.me/"

- speaker: "Jonathan Dong"
  date: May 18th, 2018
  time: 11h-12h
  room: "U/V, DMA, ENS (45 rue d'Ulm, basement)"
  affiliation: "LKB"
  title: "Scaling up Random Projections with multiple light scattering"
  abstract: "Random Projections have proven extremely useful in many signal processing and machine learning applications. However, they often require either to store a very large random matrix, or to use a different, structured matrix to reduce the computational and memory costs. We overcome this difficulty with an analog, optical device, that performs the random projections literally at the speed of light without having to store any matrix in memory. This is achieved using the physical properties of multiple coherent scattering of light in random media. These efficient optical random projections are used in two different settings: to generate Random Features for kernel approximation and to iterate an Echo-State Network (a Recurrent Neural Network with fixed internal weights). This new method is fast, power efficient and easily scalable to very large networks: we reach sizes that exceed the RAM memory limit."
  url: "http://www.lkb.upmc.fr/opticalimaging/jonathan-dong/"

- speaker: "Jean Feydy"
  date: May 4th, 2018
  time: 14h-15h
  room: "W, DMA, ENS (45 rue d'Ulm, 4th floor)"
  affiliation: "ENS, DMA"
  title: "Riemannian Geometry for Computational Anatomy"
  abstract: "How can one do statistics on datasets deprived of a suitable (0,+,*) algebraic structure?
  Can we compute the mean and PCA of a population of 3D meshes while guaranteeing the preservation of each shape's topology?<br>
  
  A Riemannian structure on a dataset (i.e. a collection of local euclidean metrics) is the most general tool allowing us to speak of \"angles\" and \"continuous straight paths\" between samples. Focusing on the case of 3D meshes endowed with a tearing-adverse structure, we will show that Riemannian priors can be enforced through the use of a generic 5-line program: the Hamiltonian shooting routine.<br>
  
  As automatic differentiation libraries now relieve us from the hassle of computing our metrics' derivatives, this talk may help you to consider using Riemannian manifolds in situations where linear models are too limited. "
  url: "http://www.math.ens.fr/~feydy/"

- speaker: "Jerome Tubiana"
  date: April 13th, 2018
  time: 10h30-11h30
  room: "W, DMA, ENS (45 rue d'Ulm, 4th floor)"
  affiliation: "ENS, LPT"
  title: "Compositional Representations in Restricted Boltzmann Machines: theory and applications."
  abstract: "Restricted Boltzmann Machines (RBM) form a family of probability distributions simple yet powerful for modeling high-dimensional, complex data sets. Besides learning a generative model, they also extract features, producing a graded and distributed representation of data. However, not all variants of RBM perform equally well, and little theoretical arguments exist for these empirical observations. By analyzing an ensemble of RBMs with random weights using statistical physics tools, we characterize the structural conditions (statistics of weights, choice of non-linearity…) allowing the emergence of such efficient representations.
  
  Lastly, we present a new application of RBMs: the analysis of protein sequences alignments. We show that RBMs extract high-order patterns of coevolution that arise from the structural and functional constraints of the protein family. These patterns can be recombined to generate artificial protein sequences with prescribed chemical properties."
  url: "https://www.lpt.ens.fr/?page=membre&id=708"

- speaker: "Aaron Tranter"
  date: March 30th, 2018
  time: 14h00-15h00
  room: "U/V, DMA, ENS (45 rue d'Ulm, basement)"
  affiliation: "Australian National University"
  title: "Deep learning cold atomic systems for quantum memories"
  abstract: "Quantum memories are integral to the realization of quantum information networks and quantum information processing. A promising platform is gradient echo memory (GEM) in cold atomic systems with demonstrated efficiencies of ~=87%. We demonstrate the first application of a deep learning algorithm to a cold atomic system in order to increase the optical depth (OD) of our atomic trap and thus increase memory efficiency. We demonstrate an improvement in OD from 530 +- 8 to 970 +- 20 by performing an optimization over 63 experimental parameters. We also observe a physical change in the atomic cloud corresponding to the spatial distribution of the atom cloud and apply the optimization to the GEM protocol."
  url: "https://physics.anu.edu.au/people/profile.php?ID=1984&tab=available_projects"

 

- speaker: "Marine Le Morvan"
  date: Mar. 16th, 2018
  time: 14h00-15h00
  room: "W, DMA, ENS (45 rue d'Ulm, 4th floor)"
  affiliation: "Mines-Paristech"
  title: "Scaling up the LASSO with interaction features"
  abstract: "Learning sparse linear models with two-way interactions is desirable in many application
  domains such as genomics. l1-regularised linear models are popular to estimate sparse models,
  yet standard implementations fail to address specifically the quadratic explosion of candidate
  two-way interactions in high dimensions, and typically do not scale to genetic data with hundreds
  of thousands of features. Here we present WHInter, a working set algorithm to solve large l1-
  regularised problems with two-way interactions for binary design matrices. The novelty of
  WHInter stems from a new bound to efficiently identify working sets while avoiding to scan
  all features, and on fast computations inspired from solutions to the maximum inner product
  search problem. We apply WHInter to simulated and real genetic data and show that it is more
  scalable and two orders of magnitude faster than the state of the art."
  url: "https://hal.archives-ouvertes.fr/search/index/q/*/authFullName_s/Marine+Le+Morvan"

- speaker: "<a href='https://tomas-angles.github.io/'>Tomas Angles</a>, <a href='https://audeg.github.io/'>Aude Genevay</a>"
  date: Feb. 16th, 2018
  time: 10h30-12h00
  room: "Room W, DMA, ENS (45 rue d'Ulm, 4th floor)"
  affiliation: "ENS (DI, DMA)"
  title: "Learning generative models beyond GANs"
  abstract: "Generative Models (i.e. high dimensional probabilistic models that are supported on low dimensional manifolds) have become a popular topic in machine learning after the now famous paper on Generative Adversarial Networks (Goodfellow et al. ‘14) that introduced an intuitive algorithm to learn those types of models to generate images resembling those of a given dataset.
  This two part tutorial will start by an introduction to generative models, and focus on some of their theoretical aspects such as structure in the latent space, and an auto-encoding perspective.
  The second part will focus on learning such models with algorithms that minimize a certain divergence between the model distribution and the ‘true’ distribution of the data. This state-of-the-art approach has a strong connection to the     original GAN algorithm but is both more stable and better formulated from a mathematical point of view."

- speaker: "Augustin Cosse"
  date: Jan. 12th, 2018
  time: 14h00-15h00
  room: "Room W, DMA, ENS (45 rue d'Ulm, 4th floor)"
  affiliation: "ENS, DMA"
  url: "http://www.augustincosse.com/"
  title: "Semidefinite programming in the era of big data"
  abstract: "Semidefinite programming (SDP) has now emerged as a powerful tool to
  derive approximations to hard problems (such as maxCut), or as a stable
  algorithm to compute exact solutions to some particular (computationally
  tractable) instances of such problems (such as in phase retrieval and
  matrix completion).
  
  The work of Shor, Nesterov, Parrilo and Lasserre brought semidefinite
  programming to another level by introducing hierarchies of semidefinite
  programs. In those hierarchies, an original polynomial optimization
  problem is approximated by a sequence of SDPs on variables of increasing
  size. Despite their theoretical interest in terms of approximability and
  stability, the use of such hierarchies is hindered by the size of the
  variables involved.

  In this talk I will address applications of semidefinite programming to
  some large scale problems from engineering and information theory. In
  particular, I will start by discussing how SDP hierarchies can be used to
  improve the current convex relaxation for rank one matrix completion which
  is based on the minimization of the nuclear norm. Going against the
  conventional wisdom, I will also discuss possible scalable numerical
  schemes for those hierarchies. As additional applications, I will briefly
  address inverse scattering as well as deconvolution and super-resolution."

- speaker: "Michael Elad"
  date: Dec. 18th, 2017
  time: 14h00-15h00
  room: "Room W, DMA, ENS (45 rue d'Ulm, 4th floor)"
  affiliation: "Technion"
  url: "http://www.cs.technion.ac.il/~elad/"
  title: "Sparse Modeling in Image Processing and Deep Learning"
  abstract: "Sparse approximation is a well-established theory, with a profound impact on the fields of signal and image processing. In this talk we start by presenting this model and its features, and then turn to describe two special cases of it – the convolutional sparse coding (CSC) and its multi-layered version (ML-CSC). Amazingly, as we will carefully show, ML-CSC provides a solid theoretical foundation to … deep-learning. Alongside this main message of bringing a theoretical backbone to deep-learning, another central message that will accompany us throughout the talk: Generative models for describing data sources enable a systematic way to design algorithms, while also providing a complete mechanism for a theoretical analysis of these algorithms' performance. This talk is meant for newcomers to this field - no prior knowledge on sparse approximation is assumed."

- speaker: "Paul Hand"
  date: Nov. 24th, 2017
  time: 14h00-15h00
  room: "Salle R, passage rouge 2e sous-sol"
  affiliation: "Rice University"
  url: "http://www.caam.rice.edu/~hand/"
  title: "Deep Compressed Sensing"
  abstract: "Combining principles of compressed sensing with deep neural network-based generative image priors has recently been empirically shown to require 10X fewer measurements than traditional compressed sensing in certain scenarios. As deep generative priors (such as those obtained via generative adversarial training) improve, analogous improvements in the performance of compressed sensing and other inverse problems may be realized across the imaging sciences. In joint work with Vladislav Voroninski, we provide a theoretical framework for studying inverse problems subject to deep generative priors. In particular, we prove that with high probability, the non-convex empirical risk objective for enforcing random deepgenerative priors subject to compressive random linear observations of the last layer of the generator has no spurious local minima, and that for a fixed network depth, these guarantees hold at order-optimal sample complexity."

- speaker: "Elizabeth Purdom"
  date: Oct. 20th, 2017
  time: 13h30-14h30
  room: "W, DMA, ENS (45 rue d'Ulm)"
  affiliation: "Berkeley"
  url: "https://www.stat.berkeley.edu/~epurdom/"
  title: "Robust strategies for analysis of single cell mRNA data"
  abstract: "mRNA sequencing of single cells is a relatively recent biological technology that allows researchers to query what genes are active in a single cell. This allows for many detailed biological queries that were not possible previously. However, because of the complexities of the experimental process, single cell mRNA-Seq results in quite noisy measurements. In this talk we will introduce for a general audience the data that is being produced and discussed the data challenges that are present in this data in the area of clustering and other forms of estimation. We will then introduce some of our strategies for the analysis of single cell mRNA-Seq."