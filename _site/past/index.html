<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="ENS-CFM Data Science Chair">

    <!-- Loading mathjax -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>


    <title>Past - ENS-CFM Data Science Chair</title>

    <link rel="canonical" href="http://localhost:4000/past/">

    <!-- Favicon -->
    <link rel="icon" type="image/png" href="/img/favicon.png">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/clean-blog.css">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <script src="/js/jquery.min.js "></script>

    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="ENS-CFM Data Science Chair" />
    
    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>


<body>

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Home</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">



                <li>
                    <a href="/seminar">Colloquium</a>
                </li>
				<li>
                    <a href="/reading_group">Reading Group</a>
                </li>
                <li>
                    <a href="/jobs">Jobs</a>
                </li>
                  <li>
                        <a href="https://challengedata.ens.fr">Challenge Data</a>
                  </li>
                  <li>
                        <a href="/coming">Coming</a>
                  </li>

                <!--

                
                <li>
                    <a href="/coming/">Coming to the seminar</a>
                </li>
                
                <li>
                    <a href="/conference-2016-12-15/">Inauguration of the Chair CFM-ENS</a>
                </li>
                
                <li>
                    <a href="/conference-2017-12-08/">Special Afternoon ''Data Science in ENS'' </a>
                </li>
                
                <li>
                    <a href="/">Data @ ENS</a>
                </li>
                
                <li>
                    <a href="/jobs/">Laplace Junior Professor Chair in Data Science</a>
                </li>
                
                <li>
                    <a href="/next/">Next</a>
                </li>
                
                <li>
                    <a href="/past/">Past</a>
                </li>
                
                <li>
                    <a href="/past_reading_group/">Past</a>
                </li>
                
                <li>
                    <a href="/reading_group/">Laplace Reading Group</a>
                </li>
                
                <li>
                    <a href="/seminar/">Data Science Colloquium</a>
                </li>
                
                <li>
                    
                </li>
                
                <li>
                    
                </li>
                
                -->
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>


    <!-- Page Header -->
<header class="intro-header" style="background-image: url('/img/ens3.png')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="site-heading">
                    <h1>Past</h1>
                    <hr class="small">
                    <span class="subheading">Past seminars</span>
                </div>
            </div>
        </div>
    </div>
</header>



<!-- Main Content -->
<div class="container">
	<div class="row">
		<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
			<p>You can find below the list of past seminars. Videos of some of the past seminars <a href="https://www.youtube.com/channel/UCAhx5LLlJDi8pTLI2EICKjQ/videos">are available online</a>.</p>

<p>
  Feb. 6th, 2018, <a href="http://www-sop.inria.fr/members/Maureen.Clerc/index.php">Maureen Clerc</a> (INRIA)
  
    [<a href="">video</a>]
  
  <br />
  <b>Title:</b> <i>Brain-computer interfaces: two concurrent learning problems</i><br />
  <b>Abstract:</b> Brain-Computer Interfaces (BCI) are systems which provide real-time interaction through brain activity, bypassing traditional interfaces such as keyboard or mouse. A target application of BCI is to restore mobility or autonomy to severely disabled patients. In BCI, new modes of perception and interaction come into play, which users must learn, just as infants learn to explore their sensorimotor system. Feedback is central in this learning. From the point of view of the system, features must be extracted from the brain activity, and translated into commands. Feature extraction and classification issues, are important components of a BCI. Adaptive learning strategies, because of the high variability of the brain signals. Moreoever, additional markers may also be extracted to modulate the system's behavior. It is for instance possible to monitor the brain's reaction to the BCI outcome.  In this talk I will present some of the current machine learning methods which are used in BCI, and the adaptation of BCI to users' needs. 
  </p>

<p>
  Nov. 14th, 2017, <a href="http://www.phys.ens.fr/~monasson/">RÃ©mi Monasson</a> (ENS)
  
    [<a href="">video</a>]
  
  <br />
  <b>Title:</b> <i>Searching for interaction networks in proteins: from statistical physics to machine learning, and back</i><br />
  <b>Abstract:</b> Over the last century, statistical physics was extremely successful to predict the collective behaviour of many physical systems from detailed knowledge about their microscopic components. However, complex systems, whose properties result from the delicate interplay of many strong and heterogenous interactions, are notoriously difficult to tackle with first-principle approaches. It is therefore tempting to use data to infer adequate  microscopic models. I will present some efforts made along this direction for proteins, based on the well-known Potts model of statistical mechanics, with an emphasis on computational and theoretical aspects. I will then show how machine learning, whose unsupervised models encompass the Potts model, can be an inspiring source of new questions for statistical mechanics.
  </p>

<p>
  Oct. 3rd, 2017, <a href="http://ece.duke.edu/faculty/guillermo-sapiro">Jean-Luc Starck</a> (CEA)
  
    [<a href="">video</a>]
  
  <br />
  <b>Title:</b> <i>Cosmostatistics: Tackling Big Data from the Sky</i><br />
  <b>Abstract:</b> Since the dawn of time, humans have been wondering about their place in the Universe.  Over the past century, advances in modern physics, technology and engineering, along with the unique possibilities offered by space missions, have opened new windows to explore the cosmos. All-sky surveys, with observations across the entire electromagnetic spectrum, are the best strategy to fully understand and model the Universe in detail. Major upcoming research facilities, such as the Large Synoptic Survey Telescope (LSST), the Square Kilometer Array (SKA) and the Euclid space telescopes will provide key elements to addressing this challenge, by producing high quality data of petabyte volumes. These surveys prove to be a major 'big data' challenge, which require the development of innovative statistical methods essential both for the data analysis and their physical interpretation. I will present some highlights of this methodology and more specifically show how novel techniques of sparsity and compressed sensing open new perspectives in analysing cosmological data. These enable us to answer fundamental questions about the nature of our Universe with impressive accuracy.
  </p>

<p>
  June 27th, 2017, <a href="http://ece.duke.edu/faculty/guillermo-sapiro">Guillermo Sapiro</a> (Duke University)
  
    [<a href="">video</a>]
  
  <br />
  <b>Title:</b> <i>Learning to Succeed while Teaching to Fail: Privacy in Closed Machine Learning Systems</i><br />
  <b>Abstract:</b> Security, privacy, and fairness have become critical in the era of data science and machine learning. More and more we see that achieving universally secure, private, and fair systems is practically impossible. We have seen for example how generative adversarial networks can be used to learn about the expected private training data; how the exploitation of additional data can reveal private information in the original one; and how what seems as unrelated features can teach us about each other. Confronted with this challenge, in this work we open a new line of research, where the security, privacy, and fairness is in a closed environment. The goal is to ensure that a given entity, trusted to infer certain information with our data, is blocked from inferring protected information from it. For example, a hospital might be allowed to produce diagnosis on the patient (the positive task), without being able to infer the irrelevant gender of the subject (negative task). Similarly, a company can guarantee they internally are not using the provided data for any undesired task, an important goal that is not contradicting the virtually impossible challenge of blocking everybody from the undesired task. We design a system that learns to perform the positive task while simultaneously being trained to fail at the negative one, and illustrate this with challenging cases where the positive task is actually harder than the negative one. The particular framework and examples open to door to security, privacy, and fairness in very important closed scenarios. Joint work with Jure Sokolic and Miguel Rodrigues.
  </p>

<p>
  Tuesday, May 16th, 2017, <a href="http://iec-lnc.ens.fr/group-for-neural-theory/members-34/faculty-in-alphabetical-order/deneve-sophie/?lang=en">Sophie Deneve</a> (ENS)
  
  
  <br />
  <b>Title:</b> <i>The brain as an optimal efficient adaptive learner</i><br />
  <b>Abstract:</b> Understanding how neural networks can learn to predict and represent time-varying variables is a fundamental challenge in neuroscience. A key complication is the error credit assignment problem: how to determine the local contribution of each synapse to the networkâs global output error. Previous work on solving this problem in spiking networks has either been restricted to linear systems (Boerlin, Machens, Deneve 2013; Bourdoukan, Deneve 2015), or to non-local learning rules (FORCE learning; Sussillo, Abbott 2009; Thalmeier et al 2016). Here we show how to learn arbitrary non-linear dynamical systems with local learning rules. Our approach uses tools from adaptive control theory, and applies them to a spiking network with nonlinear dendrites. The spiking network receives its own tracking error through feedback and learns to approximate a nonlinear dynamical systems using a purely local learning rule. The local credit assignment problem is solved because each neuron effectively contains partial information of the error made by the entire network. This error is captured by the tightly balanced voltage of each neuron. Here, a balanced network effectively acts as a predictive auto-encoder that learns to cancel its own error and feedback. The resulting network is extremely efficient in terms of the number of spikes fired, and it is highly robust to noise and neural elimination. It produces asynchronous, irregular spiking activity matching the Poisson-like neural variability observed experimentally. Our framework has several important implications. It suggests that a global learning problem, like learning to implement complex nonlinear dynamics from examples, can be solved with local rules, as long as output errors are fed back as driving input signals. Our approach inherits the analytical tools of control theory such as convergence and stability theorems that can now be applied to learning in spiking networks.
  </p>

<p>
  March. 7th, 2017, <a href="http://www.di.ens.fr/~fbach/">Francis Bach</a> (INRIA)
  
    [<a href="http://savoirs.ens.fr/expose.php?id=2937">video</a>]
  
  <br />
  <b>Title:</b> <i>Beyond stochastic gradient descent for large-scale machine learning</i><br />
  <b>Abstract:</b> Many machine learning and signal processing problems are traditionally cast as convex optimization problems. A common difficulty in solving these problems is the size of the data, where there are many observations ('large n') and each of these is large ('large p'). In this setting, online algorithms such as stochastic gradient descent which pass over the data only once, are usually preferred over batch algorithms, which require multiple passes over the data. In this talk, I will show how the smoothness of loss functions may be used to design novel algorithms with improved behavior, both in theory and practice: in the ideal infinite-data setting, an efficient novel Newton-based stochastic approximation algorithm leads to a convergence rate of O(1/n) without strong convexity assumptions, while in the practical finite-data setting, an appropriate combination of batch and online algorithms leads to unexpected behaviors, such as a linear convergence rate for strongly convex problems, with an iteration cost similar to stochastic gradient descent. (joint work with Nicolas Le Roux, Eric Moulines and Mark Schmidt).
  </p>

<p>
  Feb. 21st, 2017, <a href="http://www.yann-ollivier.org/">Yann Ollivier</a> (CNRS and Paris-Sud)
  
  
  <br />
  <b>Title:</b> <i>Intelligence artificielle et raisonnement inductif : de la thÃ©orie de l'information aux rÃ©seaux de neurones artificiels</i><br />
  <b>Abstract:</b> Les problÃ¨mes de raisonnement inductif ou d'extrapolation comme "deviner la suite d'une sÃ©rie de nombres", ou plus gÃ©nÃ©ralement, "comprendre la structure cachÃ©e dans des observations", sont fondamentaux si l'on veut un jour construire une intelligence artificielle. On a parfois l'impression que ces problÃ¨mes ne sont pas mathÃ©matiquement bien dÃ©finis. Or il existe une thÃ©orie mathÃ©matique rigoureuse du raisonnement inductif et de l'extrapolation, basÃ©e sur la thÃ©orie de l'information. Cette thÃ©orie est trÃ¨s Ã©lÃ©gante, mais difficile Ã  appliquer. <br /> En pratique aujourd'hui, ce sont les rÃ©seaux de neurones qui donnent les meilleurs rÃ©sultats sur toute une sÃ©rie de problÃ¨mes concrets d'induction et d'apprentissage (vision, reconnaissance de la parole, rÃ©cemment le jeu de Go ou les voitures sans pilote...) Je ferai le point sur quelques-uns des principes mathÃ©matiques sous-jacents et sur leur lien avec la thÃ©orie de l'information. <br /> <i>Short bio:</i> Yann Ollivier is a researcher in computer science and mathematics at the CNRS, LRI, UniversitÃ© Paris-Saclay. After starting his career in pure mathematics, with topics ranging from probability to group theory, he decided to move to artificial intelligence, with an emphasis on artificial neural network training, deep learning, and their links with information theory. In 2011 he was awarded the bronze medal of the CNRS for his work.
  </p>

<p>
  Jan. 10th, 2017, <a href="https://team.inria.fr/parietal/bertrand-thirions-page/">Bertrand Thirion</a> (INRIA and Neurospin)
  
    [<a href="http://savoirs.ens.fr/expose.php?id=2848">video</a>]
  
  <br />
  <b>Title:</b> <i>A big data approach towards functional brain mapping</i><br />
  <b>Abstract:</b> Functional neuroimaging offers a unique view on brain functional organization, which is broadly characterized by two features: the   segregation of brain territories into functionally specialized regions, and the integration of these regions into networks of coherent activity.  Functional Magnetic Resonance Imaging yields a spatially resolved, yet noisy view of this organization. It also yields useful measurements of brain integrity to compare populations and characterize brain diseases. To extract information from these data, a popular strategy is to rely on  supervised classification settings, where signal patterns are used to predict the experimental task performed by the subject during a given  experiment, which is a proxy for the cognitive or mental state of this  subject.  In this talk we will describe how the reliance on large data copora changes the picture: it boosts the generalizability of the results and provides meaningful priors to analyze novel datasets.  We will discuss the challenges posed by these analytic approaches, with an emphasis on computational aspects, and how the use of non-labelled data can be further used to improve the model learned from brain  activity data.
  </p>

<p>
  Dec. 15th, 2016, <a href="../conference-2016-12-15">Special Inauguration Conference</a> (-)
  
  
  <br />
  <b>Title:</b> <i>Inauguration of the Chair CFM-ENS</i><br />
  <b>Abstract:</b> The ENS and CFM organizes the inauguration conference of the Chair 'ModÃ¨les et Sciences des DonnÃ©es'.
  </p>

<p>
  Nov. 8th, 2016, <a href="http://tuvalu.santafe.edu/~moore/">Cristopher Moore</a> (Santa Fe Institute)
  
    [<a href="http://savoirs.ens.fr/expose.php?id=2696">video</a>]
  
  <br />
  <b>Title:</b> <i>What physics can tell us about inference?</i><br />
  <b>Abstract:</b> There is a deep analogy between statistical inference and statistical physics; I will give a friendly introduction to both of these fields. I will then discuss phase transitions in two problems of interest to a broad range of data sciences: community detection in social and biological networks, and clustering of sparse high-dimensional data. In both cases, if our data becomes too sparse or too noisy, it suddenly becomes impossible to find the underlying pattern, or even tell if there is one. Physics both helps us locate these phase transiitons, and design optimal algorithms that succeed all the way up to this point. Along the way, I will visit ideas from computational complexity, random graphs, random matrices, and spin glass theory.
  </p>

<p>
  Oct. 11th, 2016, <a href="http://cbio.mines-paristech.fr/~jvert/">Jean-Philippe Vert</a> (Mines ParisTech, Institut Curie and ENS)
  
    [<a href="http://savoirs.ens.fr/expose.php?id=2677">video</a>]
  
  <br />
  <b>Title:</b> <i>Can Big Data cure Cancer?</i><br />
  <b>Abstract:</b> As the cost and throughput of genomic technologies reach a point where DNA sequencing is close to becoming a routine exam at the clinics, there is a lot of hope that treatments of diseases like cancer can dramatically improve by a digital revolution in medicine, where smart algorithms analyze Â« big medical data Â»  to help doctors take the best decisions for each patient or to suggest new directions for drug development. While artificial intelligence and machine learning-based algorithms have indeed had a great impact on many data-rich fields, their application on genomic data raises numerous computational and mathematical challenges that I will illustrate on a few examples of patient stratification or drug response prediction from genomic data.
  </p>


		</div>
	</div>
</div>

<hr>

    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                    <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    <li>
                        <a href="https://github.com/data-ens">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <!--
                  <p class="copyright text-muted">Copyright &copy; ENS-CFM Data Science Chair 2018</p>
                -->
            </div>
        </div>
    </div>
</footer>

<!-- jQuery
<script src="/js/jquery.min.js "></script>
-->

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/clean-blog.min.js "></script>




<!-- Google analytics -->
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-781488-2";
urchinTracker();
</script>


</body>

</html>
