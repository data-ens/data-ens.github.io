<!DOCTYPE html>
<html lang="en">

<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="ENS-CFM Data Science Chair">

    <!-- Loading mathjax -->
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script>


    <title>Past - ENS-CFM Data Science Chair</title>

    <link rel="canonical" href="http://localhost:4000/past_reading_group/">

    <!-- Favicon -->
    <link rel="icon" type="image/png" href="/img/favicon.png">

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="/css/bootstrap.min.css">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="/css/clean-blog.css">

    <!-- Custom Fonts -->
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href='http://fonts.googleapis.com/css?family=Lora:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400italic,600italic,700italic,800italic,400,300,600,700,800' rel='stylesheet' type='text/css'>

    <script src="/js/jquery.min.js "></script>

    <link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="ENS-CFM Data Science Chair" />
    
    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head>


<body>

    <!-- Navigation -->
<nav class="navbar navbar-default navbar-custom navbar-fixed-top">
    <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header page-scroll">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="/">Home</a>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
            <ul class="nav navbar-nav navbar-right">



                <li>
                    <a href="/seminar">Colloquium</a>
                </li>
				<li>
                    <a href="/reading_group">Reading Group</a>
                </li>
                <li>
                    <a href="/jobs">Jobs</a>
                </li>
                  <li>
                        <a href="https://challengedata.ens.fr">Challenge Data</a>
                  </li>
                  <li>
                        <a href="/coming">Coming</a>
                  </li>

                <!--

                
                <li>
                    <a href="/coming/">Coming to the seminar</a>
                </li>
                
                <li>
                    <a href="/conference-2016-12-15/">Inauguration of the Chair CFM-ENS</a>
                </li>
                
                <li>
                    <a href="/conference-2017-12-08/">Special Afternoon ''Data Science in ENS'' </a>
                </li>
                
                <li>
                    <a href="/conference-2018-12-10/">Data Sciences in ENS</a>
                </li>
                
                <li>
                    <a href="/">Data @ ENS</a>
                </li>
                
                <li>
                    <a href="/jobs/">Laplace Junior Professor Chair in Data Science</a>
                </li>
                
                <li>
                    <a href="/next/">Next</a>
                </li>
                
                <li>
                    <a href="/past/">Past</a>
                </li>
                
                <li>
                    <a href="/past_reading_group/">Past</a>
                </li>
                
                <li>
                    <a href="/reading_group/">Laplace Reading Group</a>
                </li>
                
                <li>
                    <a href="/seminar/">Data Science Colloquium</a>
                </li>
                
                <li>
                    
                </li>
                
                <li>
                    
                </li>
                
                -->
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
    <!-- /.container -->
</nav>


    <!-- Page Header -->
<header class="intro-header" style="background-image: url('/img/ens3.png')">
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <div class="site-heading">
                    <h1>Past</h1>
                    <hr class="small">
                    <span class="subheading">Past reading groups</span>
                </div>
            </div>
        </div>
    </div>
</header>



<!-- Main Content -->
<div class="container">
	<div class="row">
		<div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
			<p>You can find below the list of past Laplace reading groups.</p>

<p>
  Nov. 8th, 2018, <a href="http://www.amensch.fr/">Arthur Mensch</a> (ENS)
  <br />
  <b>Title:</b> <i>Learning representations from functional MRI data</i><br />
  <b>Abstract:</b> Thanks to the advent of functional brain-imaging technologies, cognitive neuroscience is accumulating maps of neural activity responses to specific tasks or stimuli, or of spontaneous activity. In this work, we consider data from functional Magnetic Resonance Imaging (fMRI), that we study in a machine learning setting: we learn a model of brain activity that should generalize on unseen data. After reviewing the standard fMRI data analysis techniques, we propose new methods and models to benefit from the recently released large fMRI data repositories. Our goal is to learn richer representations of brain activity. We first focus on unsupervised analysis of terabyte-scale fMRI data acquired on subjects at rest (resting-state fMRI). We perform this analysis using matrix factorization. We present new methods for running sparse matrix factorization/dictionary learning on hundreds of fMRI records in reasonable time. Our leading approach relies on introducing randomness in stochastic optimization loops and provides speed-up of an order of magnitude on a variety of settings and datasets. We provide an extended empirical validation of our stochastic subsampling approach, for datasets from fMRI, hyperspectral imaging and collaborative filtering. We derive convergence properties for our algorithm, in a theoretical analysis that reaches beyond the matrix factorization problem. We then turn to work with fMRI data acquired on subject undergoing behavioral protocols (task fMRI). We investigate how to aggregate data from many source studies, acquired with many different protocols, in order to learn more accurate and interpretable decoding models, that predicts stimuli or tasks from brain maps. Our multi-study shared-layer model learns to reduce the dimensionality of input brain images, simultaneously to learning to decode these images from their reduced representation. This fosters transfer learning in between studies, as we learn the undocumented cognitive common aspects that the many fMRI studies share. As a consequence, our multi-study model performs better than single-study decoding. Our approach identifies universally relevant representation of brain activity, supported by a few task-optimized networks learned during model fitting. Finally, on a related topic, we show how to use dynamic programming within end-to-end trained deep networks, with applications in natural language processing.
  </p>

<p>
  July 11th, 2018, <a href="http://jpeypou.mat.utfsm.cl/">Juan Peypouquet</a> (Universidad Técnica Federico Santa María (Chile))
  <br />
  <b>Title:</b> <i>Inertial proximal algorithms for maximally monotone operators</i><br />
  <b>Abstract:</b> We present a Regularized Inertial Proximal Algorithm to solve convex optimization problems and variational inequalities. It is obtained by means of a convenient finite-difference discretization of a second-order differential equation with vanishing damping, governed by the Yosida regularization of a maximally monotone operator with time-varying index. These systems are the counterpart to accelerated forward–backward algorithms in the context of maximally monotone operators. A simple example illustrates the behavior of these systems compared with some of their close relatives.
  </p>

<p>
  June 8th, 2018, <a href="http://mblondel.org/">Mathieu Blondel</a> (NTT (Kyoto))
  <br />
  <b>Title:</b> <i>Smoothing / Regularization Techniques for Probabilistic and Structured Classification</i><br />
  <b>Abstract:</b> In this talk, I will present how smoothing / regularization techniques can be used to turn a prediction function that outputs a hard decision (e.g top-scoring class or top-scoring sequence) into a differentiable "soft" decision with a probabilistic perspective. I will present a novel principled loss family to learn such "soft" prediction functions from training data. In addition, I will show how to use them as a layer in a neural network trained end-to-end by backpropagation. Finally, I will present  experimental results on natural language processing applications such as named entity recognition, dependency parsing and machine translation. <br /> Joint work with Vlad Niculae, André Martins, Claire Cardie, Arthur Mensch <br /> The talk will be a condensed overview of the following papers: <br /> [1] https://arxiv.org/abs/1802.04223 (ICML 2018) <br /> [2] https://arxiv.org/abs/1802.03676 (ICML 2018) <br /> [3] https://arxiv.org/abs/1805.09717 (arXiv preprint)
  </p>

<p>
  June 8th, 2018, <a href="http://alberto.bietti.me/">Alberto Bietti</a> (Inria)
  <br />
  <b>Title:</b> <i>Group Invariance, Stability to Deformations, and Complexity of Deep Convolutional Representations</i><br />
  <b>Abstract:</b> The success of deep convolutional architectures is often attributed in part to their ability to learn multiscale and invariant representations of natural signals. However, a precise study of these properties and how they affect learning guarantees is still missing. In this work, we consider deep convolutional representations of signals; we study their invariance to translations and to more general groups of transformations, their stability to the action of diffeomorphisms, and their ability to preserve signal information. This analysis is carried by introducing a multilayer kernel based on convolutional kernel networks and by studying the geometry induced by the kernel mapping. We then characterize the corresponding reproducing kernel Hilbert space (RKHS), showing that it contains a large class of convolutional neural networks with homogeneous activation functions. This analysis allows us to separate data representation from learning, and to provide a canonical measure of model complexity, the RKHS norm, which controls both stability and generalization of any learned model. In addition to models in the constructed RKHS, our stability analysis also applies to convolutional networks with generic activations such as rectified linear units, and we discuss its relationship with recent generalization bounds based on spectral norms. <br /> https://arxiv.org/abs/1706.03078
  </p>

<p>
  May 18th, 2018, <a href="http://www.lkb.upmc.fr/opticalimaging/jonathan-dong/">Jonathan Dong</a> (LKB)
  <br />
  <b>Title:</b> <i>Scaling up Random Projections with multiple light scattering</i><br />
  <b>Abstract:</b> Random Projections have proven extremely useful in many signal processing and machine learning applications. However, they often require either to store a very large random matrix, or to use a different, structured matrix to reduce the computational and memory costs. We overcome this difficulty with an analog, optical device, that performs the random projections literally at the speed of light without having to store any matrix in memory. This is achieved using the physical properties of multiple coherent scattering of light in random media. These efficient optical random projections are used in two different settings: to generate Random Features for kernel approximation and to iterate an Echo-State Network (a Recurrent Neural Network with fixed internal weights). This new method is fast, power efficient and easily scalable to very large networks: we reach sizes that exceed the RAM memory limit.
  </p>

<p>
  May 4th, 2018, <a href="http://www.math.ens.fr/~feydy/">Jean Feydy</a> (ENS, DMA)
  <br />
  <b>Title:</b> <i>Riemannian Geometry for Computational Anatomy</i><br />
  <b>Abstract:</b> How can one do statistics on datasets deprived of a suitable (0,+,*) algebraic structure? Can we compute the mean and PCA of a population of 3D meshes while guaranteeing the preservation of each shape's topology?<br />
A Riemannian structure on a dataset (i.e. a collection of local euclidean metrics) is the most general tool allowing us to speak of "angles" and "continuous straight paths" between samples. Focusing on the case of 3D meshes endowed with a tearing-adverse structure, we will show that Riemannian priors can be enforced through the use of a generic 5-line program: the Hamiltonian shooting routine.<br />
As automatic differentiation libraries now relieve us from the hassle of computing our metrics' derivatives, this talk may help you to consider using Riemannian manifolds in situations where linear models are too limited. 
  </p>

<p>
  April 13th, 2018, <a href="https://www.lpt.ens.fr/?page=membre&amp;id=708">Jerome Tubiana</a> (ENS, LPT)
  <br />
  <b>Title:</b> <i>Compositional Representations in Restricted Boltzmann Machines: theory and applications.</i><br />
  <b>Abstract:</b> Restricted Boltzmann Machines (RBM) form a family of probability distributions simple yet powerful for modeling high-dimensional, complex data sets. Besides learning a generative model, they also extract features, producing a graded and distributed representation of data. However, not all variants of RBM perform equally well, and little theoretical arguments exist for these empirical observations. By analyzing an ensemble of RBMs with random weights using statistical physics tools, we characterize the structural conditions (statistics of weights, choice of non-linearity…) allowing the emergence of such efficient representations.
Lastly, we present a new application of RBMs: the analysis of protein sequences alignments. We show that RBMs extract high-order patterns of coevolution that arise from the structural and functional constraints of the protein family. These patterns can be recombined to generate artificial protein sequences with prescribed chemical properties.
  </p>

<p>
  March 30th, 2018, <a href="https://physics.anu.edu.au/people/profile.php?ID=1984&amp;tab=available_projects">Aaron Tranter</a> (Australian National University)
  <br />
  <b>Title:</b> <i>Deep learning cold atomic systems for quantum memories</i><br />
  <b>Abstract:</b> Quantum memories are integral to the realization of quantum information networks and quantum information processing. A promising platform is gradient echo memory (GEM) in cold atomic systems with demonstrated efficiencies of ~=87%. We demonstrate the first application of a deep learning algorithm to a cold atomic system in order to increase the optical depth (OD) of our atomic trap and thus increase memory efficiency. We demonstrate an improvement in OD from 530 +- 8 to 970 +- 20 by performing an optimization over 63 experimental parameters. We also observe a physical change in the atomic cloud corresponding to the spatial distribution of the atom cloud and apply the optimization to the GEM protocol.
  </p>

<p>
  Mar. 16th, 2018, <a href="https://hal.archives-ouvertes.fr/search/index/q/*/authFullName_s/Marine+Le+Morvan">Marine Le Morvan</a> (Mines-Paristech)
  <br />
  <b>Title:</b> <i>Scaling up the LASSO with interaction features</i><br />
  <b>Abstract:</b> Learning sparse linear models with two-way interactions is desirable in many application domains such as genomics. l1-regularised linear models are popular to estimate sparse models, yet standard implementations fail to address specifically the quadratic explosion of candidate two-way interactions in high dimensions, and typically do not scale to genetic data with hundreds of thousands of features. Here we present WHInter, a working set algorithm to solve large l1- regularised problems with two-way interactions for binary design matrices. The novelty of WHInter stems from a new bound to efficiently identify working sets while avoiding to scan all features, and on fast computations inspired from solutions to the maximum inner product search problem. We apply WHInter to simulated and real genetic data and show that it is more scalable and two orders of magnitude faster than the state of the art.
  </p>

<p>
  Feb. 16th, 2018, <a href=""><a href="https://tomas-angles.github.io/">Tomas Angles</a>, <a href="https://audeg.github.io/">Aude Genevay</a></a> (ENS (DI, DMA))
  <br />
  <b>Title:</b> <i>Learning generative models beyond GANs</i><br />
  <b>Abstract:</b> Generative Models (i.e. high dimensional probabilistic models that are supported on low dimensional manifolds) have become a popular topic in machine learning after the now famous paper on Generative Adversarial Networks (Goodfellow et al. ‘14) that introduced an intuitive algorithm to learn those types of models to generate images resembling those of a given dataset. This two part tutorial will start by an introduction to generative models, and focus on some of their theoretical aspects such as structure in the latent space, and an auto-encoding perspective. The second part will focus on learning such models with algorithms that minimize a certain divergence between the model distribution and the ‘true’ distribution of the data. This state-of-the-art approach has a strong connection to the     original GAN algorithm but is both more stable and better formulated from a mathematical point of view.
  </p>

<p>
  Jan. 12th, 2018, <a href="http://www.augustincosse.com/">Augustin Cosse</a> (ENS, DMA)
  <br />
  <b>Title:</b> <i>Semidefinite programming in the era of big data</i><br />
  <b>Abstract:</b> Semidefinite programming (SDP) has now emerged as a powerful tool to derive approximations to hard problems (such as maxCut), or as a stable algorithm to compute exact solutions to some particular (computationally tractable) instances of such problems (such as in phase retrieval and matrix completion).
The work of Shor, Nesterov, Parrilo and Lasserre brought semidefinite programming to another level by introducing hierarchies of semidefinite programs. In those hierarchies, an original polynomial optimization problem is approximated by a sequence of SDPs on variables of increasing size. Despite their theoretical interest in terms of approximability and stability, the use of such hierarchies is hindered by the size of the variables involved.
In this talk I will address applications of semidefinite programming to some large scale problems from engineering and information theory. In particular, I will start by discussing how SDP hierarchies can be used to improve the current convex relaxation for rank one matrix completion which is based on the minimization of the nuclear norm. Going against the conventional wisdom, I will also discuss possible scalable numerical schemes for those hierarchies. As additional applications, I will briefly address inverse scattering as well as deconvolution and super-resolution.
  </p>

<p>
  Dec. 18th, 2017, <a href="http://www.cs.technion.ac.il/~elad/">Michael Elad</a> (Technion)
  <br />
  <b>Title:</b> <i>Sparse Modeling in Image Processing and Deep Learning</i><br />
  <b>Abstract:</b> Sparse approximation is a well-established theory, with a profound impact on the fields of signal and image processing. In this talk we start by presenting this model and its features, and then turn to describe two special cases of it – the convolutional sparse coding (CSC) and its multi-layered version (ML-CSC). Amazingly, as we will carefully show, ML-CSC provides a solid theoretical foundation to … deep-learning. Alongside this main message of bringing a theoretical backbone to deep-learning, another central message that will accompany us throughout the talk: Generative models for describing data sources enable a systematic way to design algorithms, while also providing a complete mechanism for a theoretical analysis of these algorithms' performance. This talk is meant for newcomers to this field - no prior knowledge on sparse approximation is assumed.
  </p>

<p>
  Nov. 24th, 2017, <a href="http://www.caam.rice.edu/~hand/">Paul Hand</a> (Rice University)
  <br />
  <b>Title:</b> <i>Deep Compressed Sensing</i><br />
  <b>Abstract:</b> Combining principles of compressed sensing with deep neural network-based generative image priors has recently been empirically shown to require 10X fewer measurements than traditional compressed sensing in certain scenarios. As deep generative priors (such as those obtained via generative adversarial training) improve, analogous improvements in the performance of compressed sensing and other inverse problems may be realized across the imaging sciences. In joint work with Vladislav Voroninski, we provide a theoretical framework for studying inverse problems subject to deep generative priors. In particular, we prove that with high probability, the non-convex empirical risk objective for enforcing random deepgenerative priors subject to compressive random linear observations of the last layer of the generator has no spurious local minima, and that for a fixed network depth, these guarantees hold at order-optimal sample complexity.
  </p>

<p>
  Oct. 20th, 2017, <a href="https://www.stat.berkeley.edu/~epurdom/">Elizabeth Purdom</a> (Berkeley)
  <br />
  <b>Title:</b> <i>Robust strategies for analysis of single cell mRNA data</i><br />
  <b>Abstract:</b> mRNA sequencing of single cells is a relatively recent biological technology that allows researchers to query what genes are active in a single cell. This allows for many detailed biological queries that were not possible previously. However, because of the complexities of the experimental process, single cell mRNA-Seq results in quite noisy measurements. In this talk we will introduce for a general audience the data that is being produced and discussed the data challenges that are present in this data in the area of clustering and other forms of estimation. We will then introduce some of our strategies for the analysis of single cell mRNA-Seq.
  </p>


		</div>
	</div>
</div>

<hr>

    <!-- Footer -->
<footer>
    <div class="container">
        <div class="row">
            <div class="col-lg-8 col-lg-offset-2 col-md-10 col-md-offset-1">
                <ul class="list-inline text-center">

                    <li>
                        <a href="/feed.xml">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-rss fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    
                    <li>
                        <a href="https://github.com/data-ens">
                            <span class="fa-stack fa-lg">
                                <i class="fa fa-circle fa-stack-2x"></i>
                                <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
                <!--
                  <p class="copyright text-muted">Copyright &copy; ENS-CFM Data Science Chair 2019</p>
                -->
            </div>
        </div>
    </div>
</footer>

<!-- jQuery
<script src="/js/jquery.min.js "></script>
-->

<!-- Bootstrap Core JavaScript -->
<script src="/js/bootstrap.min.js "></script>

<!-- Custom Theme JavaScript -->
<script src="/js/clean-blog.min.js "></script>




<!-- Google analytics -->
<script src="http://www.google-analytics.com/urchin.js" type="text/javascript">
</script>
<script type="text/javascript">
_uacct = "UA-781488-2";
urchinTracker();
</script>


</body>

</html>
