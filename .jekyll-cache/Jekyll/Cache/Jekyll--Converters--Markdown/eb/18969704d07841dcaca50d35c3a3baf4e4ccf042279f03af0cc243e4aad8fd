I"ü<p>Welcome to the Data Science Colloquium of the <a href="http://www.ens.fr/">ENS</a>.</p>

<p>This colloquium is organized around data sciences in a broad sense
with the goal of bringing together researchers with diverse
backgrounds (including for instance mathematics, computer science,
physics, chemistry and neuroscience) but a common interest in dealing
with large scale or high dimensional data.</p>

<p>The colloquium is followed by an open buffet around which participants can meet and discuss collaborations.</p>

<p>These seminars are made possible by the support of the <a href="https://data-ens.github.io">CFM-ENS
Chair ‚Äú<em>Mod√®les et Sciences des Donn√©es</em>‚Äù</a>.</p>

<p>You can check the list of the next seminars below and the <a href="../past/">list of past seminars</a>.</p>

<p>Videos of some of the past seminars <a href="https://www.youtube.com/channel/UCAhx5LLlJDi8pTLI2EICKjQ/videos">are available online</a>.</p>

<h1 id="organizers">Organizers</h1>

<p>The colloquium is organized by:</p>

<ul>
  <li><a href="https://www.ipht.fr/Pisp/giulio.biroli/cours.php">Giulio Biroli</a> (ENS): director ;</li>
  <li><a href="https://www.di.ens.fr/~mallat/">St√©phane Mallat</a> (Coll√®ge de France) ;</li>
  <li><a href="https://lsp.dec.ens.fr/fr/member/646/christian-lorenzi">Christian Lorenzi</a> (CNRS and ENS) ;</li>
  <li><a href="http://gpeyre.github.io/">Gabriel Peyr√©</a> (CNRS and ENS).</li>
</ul>

<h1 id="next-seminars">Next seminars</h1>

<p>
  Thurs. Oct. 21th, 2021, 10h45-11h45 (Paris time), room Amphi Jaures (29 Rue d'Ulm).<br />
  <a href="https://wp.nyu.edu/courantinstituteofmathematicalsciences-eve2/">Eric Vanden-Eijnden</a>  (NYU)<br />
  <b>Title:</b> <i>Machine learning and applied mathematics</i><br />
  <b>Abstract:</b> The recent success of machine learning suggests that neural networks may be capable of approximating high-dimensional functions with controllably small errors. As a result, they could outperform standard function interpolation methods that have been the workhorses of scientific computing but do not scale well with dimension. In support of this prospect, here I will review what is known about the trainability and accuracy of shallow neural networks, which offer the simplest instance of nonlinear learning  in functional spaces that are fundamentally different from classic approximation spaces. The dynamics of training in these spaces can be analyzed using tools from optimal transport and statistical mechanics, which reveal when and how shallow neural networks can overcome the curse of dimensionality. I will also discuss how scientific computing problem in high-dimension once thought intractable can be revisited through the lens of these results, focusing on applications related to (i) solving Fokker-Planck equations associated with high-dimensional systems displaying metastability and (ii) sampling Boltzmann-Gibbs distributions using generative models to assist MCMC methods.
  </p>

:ET